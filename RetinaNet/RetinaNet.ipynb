{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetinaNet Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please Note:** For better usability `Training` and `Testing` sections should be placed in seperate source files. Furtheremore some functions like `getModel` are defined twice. Once in each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, to install PyTorch the command below was used since it download `PyTroch with Cuda compatability` which allowed the model to run on GPU instead of CPU which `helped decrease execution time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VisionDataset\n",
    "from torchvision.models.detection import RetinaNet_ResNet50_FPN_Weights\n",
    "from torchvision.transforms import functional\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc, confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section helps prepare data and configuring a RetinaNet model. The `CLASS_MAPPING` dictionary assigns numerical IDs to object classes, mapping them as follows: `Background to 0`, `Mixed Waste - Black Bag to 1`, `Organic Waste - White Bag to 2`, `Other to 3`, and `Recycled Waste - Grey or Green Bag to 4`. This mapping is needed for converting textual class labels into numerical IDs required by the model.\n",
    "\n",
    "Furthermore, `get_retinanet_model` function initializes a pre-trained RetinaNet model with default weights (RetinaNet_ResNet50_FPN_Weights.DEFAULT) and customizes it for the number of classes in the dataset. The function also updates the model's classification head by configuring its output layer to handle the specified number of classes and adjusts the num_classes attribute for dataset compatability.\n",
    "\n",
    "Additionally, the `VOCTransform` class processes images and their annotations from the VOC dataset format. It converts images to PyTorch tensors and parses  the bounding boxes and class labels from the XML files. Bounding box coordinates are validated and class labels are mapped to numerical IDs using the CLASS_MAPPING dictionary. The transformed data is returned as PyTorch-compatible tensors.\n",
    "\n",
    "Lastly for this section. The `VOCDataset` class is designed to load a dataset following the VOC format. When called, it sorts the fils into image files (.jpg) and their corresponding annotation files (.xml). The `__getitem__` method retrieves an image and its annotation based on an index, iterates through the XML file to extract bounding box coordinates and labels, and applies optional transformations. The `__len__` method returns the total number of samples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Mapping\n",
    "CLASS_MAPPING = {\n",
    "    \"Background\": 0,\n",
    "    \"Mixed Waste -Black Bag-\": 1,\n",
    "    \"Organic Waste -White Bag-\": 2,\n",
    "    \"Other\": 3,\n",
    "    \"Recycled Waste -Grey or Green Bag-\": 4\n",
    "}\n",
    "\n",
    "\n",
    "# * ##############################################################################################################\n",
    "\n",
    "\n",
    "# Function to get RetinaNet model\n",
    "def get_retinanet_model(num_classes):\n",
    "    # Load pre-trained RetinaNet model\n",
    "    model = torchvision.models.detection.retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "    # Update the number of classes in the classification head\n",
    "    in_features = model.head.classification_head.cls_logits.in_channels\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "    \n",
    "    # Update classification head\n",
    "    model.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
    "        in_features, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
    "    )\n",
    "    \n",
    "    # Update number of classes\n",
    "    model.head.classification_head.num_classes = num_classes\n",
    "    \n",
    "    # Return model\n",
    "    return model\n",
    "\n",
    "# Class to transform VOC XML annotations to PyTorch tensors\n",
    "class VOCTransform:\n",
    "    def __call__(self, image, target):\n",
    "        # Convert PIL image to tensor\n",
    "        image = functional.to_tensor(image)\n",
    "\n",
    "        # Parse bounding boxes and labels from XML\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        # Extract bounding boxes and labels from XML (target)\n",
    "        for obj in target['annotation']['object']:\n",
    "            bbox = obj['bndbox']\n",
    "            xmin = float(bbox['xmin'])\n",
    "            ymin = float(bbox['ymin'])\n",
    "            xmax = float(bbox['xmax'])\n",
    "            ymax = float(bbox['ymax'])\n",
    "\n",
    "            # Validate bounding boxes\n",
    "            if xmax <= xmin or ymax <= ymin:\n",
    "                print(f\"Invalid bounding box detected: {bbox}\")\n",
    "                continue\n",
    "            \n",
    "            # Append bounding boxes\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # Map class name to numerical ID\n",
    "            class_name = obj['name'] # get class name\n",
    "            \n",
    "            # If class name is in the mapping\n",
    "            if class_name in CLASS_MAPPING:\n",
    "                # Append the numerical ID\n",
    "                labels.append(CLASS_MAPPING[class_name])\n",
    "            else:\n",
    "                # Error\n",
    "                raise ValueError(f\"Unknown class name: {class_name}\")\n",
    "\n",
    "        # Filter out invalid bounding boxes before creating tensors\n",
    "        if len(boxes) == 0 or len(labels) == 0:\n",
    "            print(f\"Warning: Skipping image due to invalid annotations.\")\n",
    "            return None, None\n",
    "\n",
    "        # Convert target to PyTorch tensors\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        \n",
    "        # Return transformed image and target\n",
    "        return image, target\n",
    "\n",
    "# Class to load VOC dataset\n",
    "class VOCDataset(VisionDataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        super().__init__(root, transforms=transforms)   # Initialise parent class\n",
    "        self.images = sorted([f for f in os.listdir(root) if f.endswith(\".jpg\")])   # Get sorted list of all images in dataset\n",
    "        self.annotations = sorted([f for f in os.listdir(root) if f.endswith(\".xml\")]) # Get sorted list of all annotations in dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.root, self.images[index])    # Get image path\n",
    "        annotation_path = os.path.join(self.root, self.annotations[index]) # Get annotation path\n",
    "\n",
    "        # Load image\n",
    "        image = functional.to_pil_image(torchvision.io.read_image(image_path)) # Read image and convert to PIL image\n",
    "\n",
    "        # Load annotation\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot() \n",
    "\n",
    "        # Parse annotation into dictionary\n",
    "        target = {\"annotation\": {}}             # Empty dictionary to store annotation key\n",
    "        target['annotation']['object'] = []     # Empty list to store objects\n",
    "        \n",
    "        # Iterate over all 'object' elements in the XML root\n",
    "        for obj in root.findall('object'):\n",
    "            \n",
    "            # Create dictionary for each object with name and bounding box coordinates\n",
    "            obj_struct = {\n",
    "                \"name\": obj.find(\"name\").text,\n",
    "                \"bndbox\": {\n",
    "                    \"xmin\": obj.find(\"bndbox/xmin\").text,\n",
    "                    \"ymin\": obj.find(\"bndbox/ymin\").text,\n",
    "                    \"xmax\": obj.find(\"bndbox/xmax\").text,\n",
    "                    \"ymax\": obj.find(\"bndbox/ymax\").text,\n",
    "                },\n",
    "            }\n",
    "            \n",
    "            # Append object dictionary to the target dictionary\n",
    "            target['annotation']['object'].append(obj_struct)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "            \n",
    "            # If image or target is invalid\n",
    "            if image is None or target is None:\n",
    "                # Skip invalid samples\n",
    "                return self.__getitem__((index + 1) % len(self))\n",
    "\n",
    "        # Return image and target\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of samples in dataset\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training One Epoch Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_one_epoch` function trains the RetinaNet model for one epoch using the provided `data_loader`, `optimizer`, and `device`. It sets the model to `training mode` with `model.train()` and initializes total_loss to track the cumulative loss.\n",
    "\n",
    "For each batch, images and targets are moved to the specified device, and the model calculates the losses. The losses are summed, and any NaN values trigger an early exit with an error message. Gradients are then reset using optimizer.zero_grad(), and backpropagation is performed with losses.backward(). To prevent exploding gradients, they are clipped to a maximum norm of 10.0, followed by updating the model parameters with optimizer.step().\n",
    "\n",
    "The average loss over all batches in the epoch is then calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train one epoch\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    # Initialize total loss\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over all batches and extract images and targets\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        # Output progress\n",
    "        print(f\"Epoch: {epoch + 1} | Batch: {batch_idx + 1}/{len(data_loader)}\", end='\\r')\n",
    "\n",
    "        # Move images and targets to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Debugging loss incase of NaN values\n",
    "        if torch.isnan(losses).any():\n",
    "            print(f\"NaN Detected in Loss at Epoch {epoch + 1}, Batch {batch_idx + 1}\")\n",
    "            return\n",
    "        \n",
    "        # Update total loss\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        # Perform backpropagation and optimizer step\n",
    "        optimizer.zero_grad()   # Reset gradients for all model parameters to zero\n",
    "        losses.backward()       # Compute gradients by performing backpropagation using the loss\n",
    "\n",
    "        # Clip gradients to a maximum norm of 10.0 prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()        # Update model parameters using the computed gradients\n",
    "\n",
    "    # Calculate average loss over all batches and display\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch: {epoch + 1} | Batch: {len(data_loader)}/{len(data_loader)} | Average Loss (over all batches in epoch): {average_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Dataset and Creating DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_dataset` is an instance of the VOCDataset class comprised of images and targets. The transforms=VOCTransform() parameter applies the VOCTransform class to preprocess the images and annotations, converting them into PyTorch-compatible tensors. This dataset serves as the source for the DataLoader during training.\n",
    "\n",
    "The `train_loader` loads data from the train_dataset, with each batch containing 8 samples (batch_size=8). The shuffle=True parameter ensures the data is randomized at the start of each epoch to improve generalization and the collate_fn=lambda x: tuple(zip(*x)) organizes each batch into separate lists of images and targets, making them compatible with the model's input requirements. This setup allows for smooth and efficient data handling during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- <Loading Training Dataset> -----\n",
      "----- <Training Dataset Loaded Successfully - 1215 Training Samples> -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset and Create DataLoader\n",
    "print(\"----- <Loading Training Dataset> -----\")\n",
    "train_dir = \"./trashy-dataset-roboflow.voc/train\"\n",
    "\n",
    "train_dataset = VOCDataset(train_dir, transforms=VOCTransform())\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "print(f\"----- <Training Dataset Loaded Successfully - {len(train_dataset)} Training Samples> -----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load RetinaNet Architecture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes the RetinaNet model for 5 classes (including the background) using `get_retinanet_model`. It checks for GPU availability and sets the device to cuda if available; otherwise, it defaults to cpu. The model is then moved to the chosen device, ensuring compatibility with the hardware for training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- <Loading RetinaNet Model Architecture> -----\n",
      "NVIDIA GeForce RTX 3070 Ti Laptop GPU Found - Moving to GPU\n",
      "----- <Model Loaded and Moved Successfully> -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of classes (including background)\n",
    "num_classes = 5  # Background, Mixed Waste, Organic Waste, Other, Recycled Waste\n",
    "\n",
    "# Get RetinaNet model\n",
    "print(\"----- <Loading RetinaNet Model Architecture> -----\")\n",
    "model = get_retinanet_model(num_classes)\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    # output message that GPU is available and display the device name\n",
    "    print(f\"{torch.cuda.get_device_name(0)} Found - Moving to GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # output message that GPU is not available\n",
    "    print(\"GPU Not Found - Moving to CPU\")\n",
    "    \n",
    "# Move model to device\n",
    "model.to(device)\n",
    "print(\"----- <Model Loaded and Moved Successfully> -----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters and Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines the training process for the RetinaNet model. An `SGD optimizer` is configured with a learning rate of 0.001, momentum of 0.9, and weight decay of 0.0005, while a `StepLR scheduler` reduces the learning rate by a factor of 0.1 every 3 epochs. \n",
    "\n",
    "The training loop runs for 100 epochs, calling `train_one_epoch` to train the model for each epoch and updating the learning rate using the scheduler. At the final epoch, the model's weights are saved to a specified directory, creating it if it doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- <Training Model> -----\n",
      "Epoch: 1 | Batch: 152/152 | Average Loss (over all batches in epoch): 39.003\n",
      "Epoch: 2 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.525\n",
      "Epoch: 3 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.443\n",
      "Epoch: 4 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.383\n",
      "Epoch: 5 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.371\n",
      "Epoch: 6 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.363\n",
      "Epoch: 7 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.356\n",
      "Epoch: 8 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.355\n",
      "Epoch: 9 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.355\n",
      "Epoch: 10 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 11 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 12 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 13 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 14 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 15 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 16 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 17 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 18 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 19 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 20 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 21 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 22 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 23 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 24 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 25 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 26 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 27 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 28 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 29 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 30 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 31 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 32 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 33 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 34 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 35 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 36 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 37 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 38 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 39 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 40 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 41 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 42 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 43 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 44 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 45 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 46 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 47 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 48 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 49 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 50 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 51 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 52 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 53 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 54 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 55 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 56 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 57 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 58 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 59 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 60 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 61 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 62 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 63 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 64 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 65 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 66 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 67 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 68 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 69 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 70 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 71 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 72 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 73 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 74 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 75 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 76 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 77 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 78 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 79 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 80 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 81 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 82 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 83 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 84 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 85 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 86 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 87 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 88 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 89 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 90 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 91 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 92 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 93 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 94 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 95 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 96 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 97 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 98 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 99 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "Epoch: 100 | Batch: 152/152 | Average Loss (over all batches in epoch): 0.354\n",
      "\n",
      "Model saved at epoch 100 in ./RetinaNet-Weights\n",
      "----- <Model Trained Successfully> -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1) # Reduce learning rate by a factor of 0.1 every 3 epochs\n",
    "\n",
    "# Training loop\n",
    "print(\"----- <Training Model> -----\")\n",
    "\n",
    "num_epochs = 100\n",
    "model_save_path = \"./RetinaNet-Weights\"\n",
    "\n",
    "# Iterate over all epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Train epoch\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # If last epoch\n",
    "    if epoch + 1 == num_epochs:\n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(model_save_path):\n",
    "            os.makedirs(model_save_path)\n",
    "            \n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_path, f\"retinanet_epoch_{epoch + 1}.pth\"))\n",
    "        print(f\"\\nModel saved at epoch {epoch + 1} in {model_save_path}\")\n",
    "        \n",
    "# Completion message\n",
    "print(\"----- <Model Trained Successfully> -----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this section helps prepare data and configuring a RetinaNet model. The `CLASS_MAPPING` dictionary assigns numerical IDs to object classes, mapping them as follows: `Background to 0`, `Mixed Waste - Black Bag to 1`, `Organic Waste - White Bag to 2`, `Other to 3`, and `Recycled Waste - Grey or Green Bag to 4`. This mapping is needed for converting textual class labels into numerical IDs required by the model.\n",
    "\n",
    "Furthermore, `get_retinanet_model` function initializes a pre-trained RetinaNet model with default weights (RetinaNet_ResNet50_FPN_Weights.DEFAULT) and customizes it for the number of classes in the dataset. The function also updates the model's classification head by configuring its output layer to handle the specified number of classes and adjusts the num_classes attribute for dataset compatability.\n",
    "\n",
    "The `preprocess_image` function prepares an image for inference. It opens the image from the given path, converts it to RGB, transforms it into a PyTorch tensor, adds a batch dimension, and moves it to the specified device (GPU or CPU).\n",
    "\n",
    "The `load_ground_truth` function processes annotation files in the dataset directory. For each XML file, it gets the class labels from the annotations using the CLASS_MAPPING dictionary. These labels are associated with their corresponding image filenames in a dictionary. If an unrecognised class name is encountered, a warning is printed. The function returns a dictionary mapping image filenames to their ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class Mapping - ALREADY DEFINED ABOVE\n",
    "# CLASS_MAPPING = {\n",
    "#     \"Background\": 0,\n",
    "#     \"Mixed Waste -Black Bag-\": 1,\n",
    "#     \"Organic Waste -White Bag-\": 2,\n",
    "#     \"Other\": 3,\n",
    "#     \"Recycled Waste -Grey or Green Bag-\": 4\n",
    "# }\n",
    "\n",
    "\n",
    "# * ######################################################################################################################\n",
    "\n",
    "\n",
    "# # Function to get RetinaNet model - ALREADY DEFINED ABOVE\n",
    "# def get_retinanet_model(num_classes):\n",
    "#     # Load pre-trained RetinaNet model\n",
    "#     model = torchvision.models.detection.retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "#     # Update the number of classes in the classification head\n",
    "#     in_features = model.head.classification_head.cls_logits.in_channels\n",
    "#     num_anchors = model.head.classification_head.num_anchors\n",
    "    \n",
    "#     # Update classification head\n",
    "#     model.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
    "#         in_features, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
    "#     )\n",
    "    \n",
    "#     # Update number of classes\n",
    "#     model.head.classification_head.num_classes = num_classes\n",
    "    \n",
    "#     # Return model\n",
    "#     return model\n",
    "\n",
    "# Function to preprocess image\n",
    "def preprocess_image(img_path, device):\n",
    "    # Open image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Convert image to tensor and add batch dimension\n",
    "    img_tensor = functional.to_tensor(img).unsqueeze(0)\n",
    "    \n",
    "    # move image to device and return it\n",
    "    return img_tensor.to(device)\n",
    "\n",
    "# Function to load ground truth from XML files in dataset directory\n",
    "def load_ground_truth(dataset_dir, CLASS_MAPPING):\n",
    "    ground_truth = {}\n",
    "    \n",
    "    # Iterate through all file in dataset directory\n",
    "    for file in os.listdir(dataset_dir):\n",
    "        # Check if file is a XML (annotation) file\n",
    "        if file.endswith('.xml'):\n",
    "            # Pase filr and extract structure\n",
    "            tree = ET.parse(os.path.join(dataset_dir, file))\n",
    "            root = tree.getroot()  # Get the root element of the XML tree\n",
    "\n",
    "            # Empty list for labels\n",
    "            labels = []\n",
    "            \n",
    "            # Iterate through all objects in the XML file\n",
    "            for obj in root.findall('object'):\n",
    "                # Extract class name\n",
    "                class_name = obj.find('name').text\n",
    "                \n",
    "                # Map class name to class ID and append to labels\n",
    "                if class_name in CLASS_MAPPING:\n",
    "                    labels.append(CLASS_MAPPING[class_name])\n",
    "                else:\n",
    "                    # Error\n",
    "                    print(f\"Warning: Unknown class '{class_name}' in {file}\")\n",
    "                    \n",
    "            # Extract image name\n",
    "            image_name = root.find('filename').text\n",
    "            # Add labels to ground truth dictionary\n",
    "            ground_truth[image_name] = labels\n",
    "    \n",
    "    # return ground truth dictionary\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_precision_recall` function calculates and visualizes precision-recall curves for each class, along with their `AUC (Area Under Curve)` values. It uses predictions (y_scores) and true labels (y_true), skipping classes without positive examples. The precision, recall, and AUC for each class are plotted and saved as an image in the specified output_dir.\n",
    "\n",
    "The `plot_multiclass_confusion_matrix` function generates a confusion matrix to evaluate the model's performance across classes. It maps numerical predictions and ground truth labels to class names using `CLASS_MAPPING`. The confusion matrix is computed, optionally normalized, and displayed as a heatmap with annotations. The plot is saved to the output directory, offering insights into classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot precision-recall curve and calculate AUC\n",
    "def plot_precision_recall(y_true, y_scores, num_classes, output_dir, CLASS_MAPPING):\n",
    "    # Output progress\n",
    "    print(\"Plotting Precision-Recall Curve and Calculating AUC\", end='\\r')\n",
    "    \n",
    "    # Convert dictionary keys to a list for consistent indexing\n",
    "    class_names = list(CLASS_MAPPING.keys())\n",
    "\n",
    "    # Set up the figure size for the plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Initialize variables for micro-average\n",
    "    y_true_flat = y_true.ravel()\n",
    "    y_scores_flat = y_scores.ravel()\n",
    "\n",
    "    # Compute the micro-average Precision-Recall curve\n",
    "    precision_micro, recall_micro, _ = precision_recall_curve(y_true_flat, y_scores_flat)\n",
    "\n",
    "    # Calculate the micro-average AUC\n",
    "    micro_auc = auc(recall_micro, precision_micro)\n",
    "\n",
    "    # Plot the micro-average curve\n",
    "    plt.plot(recall_micro, precision_micro, linestyle='--', color='mediumblue', linewidth=2.5,\n",
    "             label=f\"Overall Micro-average (AUC = {micro_auc:.3f})\")\n",
    "\n",
    "    # Loop through each class and compute its precision-recall curve\n",
    "    for class_id in range(num_classes):\n",
    "        class_name = class_names[class_id]\n",
    "        # Check if there are any positive examples for the class\n",
    "        if np.sum(y_true[:, class_id]) == 0:\n",
    "            print(f\"Warning: No positive samples found for class '{class_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Compute precision, recall, and thresholds for the class\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, class_id], y_scores[:, class_id])\n",
    "\n",
    "        # Calculate the area under the precision-recall curve (AUC)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Plot the precision-recall curve for the class, including the AUC in the label\n",
    "        plt.plot(recall, precision, label=f\"{class_name} (AUC = {pr_auc:.3f})\")\n",
    "\n",
    "    # Label the axes\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"RetinaNet - Precision-Recall Curve\")\n",
    "    # Add a legend to identify the curves by class\n",
    "    plt.legend(loc=\"best\")\n",
    "    # Add a grid for better readability\n",
    "    plt.grid()\n",
    "    # Save the plot as an image file\n",
    "    plt.savefig(f\"{output_dir}/precision_recall_curve_with_auc.png\")\n",
    "    # Close the plot to free up memory\n",
    "    plt.close('all')\n",
    "    \n",
    "    print(\"Plotting Precision-Recall Curve and Calculating AUC | Done\")\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_multiclass_confusion_matrix(y_true, y_pred, num_classes, output_dir, CLASS_MAPPING, normalize=False):\n",
    "    # Output progress\n",
    "    print(\"Plotting Confusion Matrix\", end='\\r')\n",
    "    \n",
    "    # Reverse CLASS_MAPPING to get class labels\n",
    "    class_names = list(CLASS_MAPPING.keys())\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(\n",
    "        y_true.argmax(axis=1),\n",
    "        y_pred.argmax(axis=1),\n",
    "        labels=range(num_classes)\n",
    "    )\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / (cm.sum(axis=1, keepdims=True) + 1e-10)\n",
    "        cm = np.nan_to_num(cm)  # Replace NaN with 0 if division by 0 occurs\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    # Create a heatmap for the confusion matrix\n",
    "    heatmap = sns.heatmap(cm, \n",
    "                          annot=True, \n",
    "                          fmt='.2f' if normalize else 'd', \n",
    "                          cmap='Blues', \n",
    "                          xticklabels=class_names, \n",
    "                          yticklabels=class_names,\n",
    "                          annot_kws={\"size\": 14})  # Annotation font size\n",
    "\n",
    "    # Add titles and axis labels\n",
    "    plt.title(\"RetinaNet - Confusion Matrix\", fontsize=18)\n",
    "    plt.xlabel(\"Predicted Labels\", fontsize=14)\n",
    "    plt.ylabel(\"True Labels\", fontsize=14)\n",
    "    # Rotate the x-axis labels for better visibility\n",
    "    plt.xticks(rotation=30, ha='right', fontsize=12)\n",
    "    plt.yticks(rotation=0, fontsize=12)\n",
    "    # Add a color bar label\n",
    "    colorbar = heatmap.collections[0].colorbar\n",
    "    colorbar.set_label(\"Count\" if not normalize else \"Proportion\", fontsize=12)\n",
    "    # Automatically adjust the layout to avoid truncation\n",
    "    plt.tight_layout()\n",
    "    # Save the plot\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrix_multiclass.png\"))\n",
    "    plt.close('all')\n",
    "    \n",
    "    print(\"Plotting Confusion Matrix | Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Draw Bounding Boxes on Test Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `draw_bboxes` function draws predicted bounding boxes, labels, and confidence scores on an image and saves the result. It gets the bounding box coordinates, class labels, and prediction scores from the prediction object. For each valid prediction, the function draws a bounding box and annotates it with the class name and confidence score. The image is saved to the specified output_dir, and progress is displayed during the process. This function helps visually evaluate the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw bounding boxes on images\n",
    "def draw_bboxes(output_dir, image, image_name, prediction, fig_size, CLASS_MAPPING, saved_images_counter, total_images):\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy() # get predicted bounding boxes\n",
    "    labels = prediction[0]['labels'].cpu().numpy() # get predicted labels\n",
    "    scores = prediction[0]['scores'].cpu().numpy() # get predicted scores\n",
    "\n",
    "    # Set a threshold for showing bounding boxes\n",
    "    threshold = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Draw bboxes\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        # If score is below threshold, ignore\n",
    "        if score > threshold:\n",
    "            # Get box coordinates\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            # Get class name from mapping - Switching from IDs to class names\n",
    "            class_name = CLASS_MAPPING.get(label)\n",
    "\n",
    "            # Draw bbox\n",
    "            ax.add_patch(\n",
    "                plt.Rectangle(\n",
    "                    (x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                    fill=False, edgecolor='red', linewidth=2\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Add class name and confidence score\n",
    "            ax.text(\n",
    "                x_min, y_min,\n",
    "                f'{class_name} ({score:.3f})',\n",
    "                color='blue',\n",
    "                fontsize=10,\n",
    "            )\n",
    "    \n",
    "    # Remove axis\n",
    "    ax.axis('off')\n",
    "    # Save image\n",
    "    fig.savefig(f'{output_dir}/{image_name}.png', bbox_inches='tight', pad_inches=0)\n",
    "    \n",
    "    # Display progress\n",
    "    if (saved_images_counter + 1) == (total_images - 1):\n",
    "        print(f\"Saved image {saved_images_counter + 1}/{total_images - 1}\")\n",
    "    else:\n",
    "        print(f\"Saved image {saved_images_counter + 1}/{total_images - 1}\", end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "fig_size = (8, 8)\n",
    "# Num of classes\n",
    "numOfClasses = 5 # Trash, Mixed Waste -Black Bag-, Organic Waste -White Bag-, Other, Recycled Waste -Grey or Green Bag-\n",
    "testing_dir = './trashy-dataset-roboflow.voc/test'  # Get testing directory\n",
    "output_dir_images = './images'                      # Output directories for images\n",
    "output_dir_plots = './plots'                        # Output directories for plots\n",
    "saved_model_dir = './RetinaNet-Weights'             # Saved model directory\n",
    "# saved images counter\n",
    "saved_images_counter = 0\n",
    "\n",
    "# Initialize lists for true labels and predicted scores\n",
    "y_true_list = []\n",
    "y_scores_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for Device Availability and Loading Weights From Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads a trained RetinaNet model from a specified directory and moves it to the appropriate device (GPU or CPU). The model path is determined dynamically since during training only the last epoch is saved effectivly having only one model saved instead of multiple models, selecting the first file in the `saved_model_dir`. If a GPU is available, the model is moved to the GPU; otherwise, it is loaded onto the CPU. The model is initialized using `get_retinanet_model`, and its weights are loaded from the saved state dictionary. The model is then moved to the selected device, set to `evaluation mode with model.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- <Loading Model> -----\n",
      "GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU is available - moving model to GPU\n",
      "----- <Model [retinanet_epoch_100.pth] Loaded and Moved Successfully> -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Load Model and Move to Device\n",
    "print(\"\\n----- <Loading Model> -----\")\n",
    "model_path = os.path.join(saved_model_dir, os.listdir(saved_model_dir)[0])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available - moving model to GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"No GPU available. Moving testing to CPU\")\n",
    "\n",
    "model = get_retinanet_model(numOfClasses)\n",
    "state_dict = torch.load(model_path, weights_only=True, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"----- <Model [{os.listdir(saved_model_dir)[0]}] Loaded and Moved Successfully> -----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code in this sections tests the trained RetinaNet model on images from a testing directory. It starts by creating an output directory for saving annotated images. Ground truth labels are loaded using `load_ground_truth`, and for each test image, the following steps are performed:\n",
    "\n",
    "- **Preprocessing:** The image is preprocessed into a tensor using preprocess_image and moved to the device.\n",
    "- **Ground Truth Conversion:** True labels are converted into a one-hot encoded format for precision-recall calculations.\n",
    "- **Prediction:** The model predicts bounding boxes, class labels, and scores for the image using torch.no_grad() for efficiency.\n",
    "- **Score Collection:** Predicted scores are stored for later evaluation.\n",
    "- **Visualization:** Bounding boxes and labels are drawn on the image using draw_bboxes, and the result is saved to the output directory.\n",
    "\n",
    "The process iterates over all test images, saving predictions and displaying progress, with memory management ensured by closing all plots after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- <Testing Model> -----\n",
      "\n",
      "Saved image 48/48\n",
      "Results Saved in ./images Folder\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"----- <Testing Model> -----\\n\")\n",
    "    \n",
    "# Create output image directory\n",
    "if not os.path.exists(output_dir_images):\n",
    "    os.makedirs(output_dir_images)\n",
    "\n",
    "# Get all images in testing directory\n",
    "test_images = [img for img in os.listdir(testing_dir) if img.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth = load_ground_truth(testing_dir, CLASS_MAPPING)\n",
    "\n",
    "# Iterate through all images\n",
    "for image in test_images:\n",
    "    # Get ground truth labels for the image\n",
    "    true_labels = ground_truth.get(image, [0])  # Default to background if no labels\n",
    "    \n",
    "    # Get image path\n",
    "    img_path = os.path.join(testing_dir, image)\n",
    "    # Preprocess image\n",
    "    image_tensor = preprocess_image(img_path, device)\n",
    "    \n",
    "    # Convert to one-hot encoding for precision-recall computation\n",
    "    true_one_hot = np.zeros(numOfClasses)\n",
    "    \n",
    "    for label in true_labels:\n",
    "        true_one_hot[label] = 1\n",
    "\n",
    "    y_true_list.append(true_one_hot)\n",
    "\n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        # Get model prediction\n",
    "        prediction = model(image_tensor)\n",
    "        \n",
    "        # Collect predicted scores\n",
    "        pred_scores = np.zeros(numOfClasses)\n",
    "        \n",
    "        for label, score in zip(prediction[0]['labels'].cpu().numpy(), prediction[0]['scores'].cpu().numpy()):\n",
    "            pred_scores[label] = max(pred_scores[label], score)\n",
    "\n",
    "        # Add background scores if not explicitly included\n",
    "        pred_scores[0] = 1 - np.sum(pred_scores[1:])  # Assume background is the complement of other scores\n",
    "\n",
    "        # Append the predicted scores to the y_scores_list for later metric computation\n",
    "        y_scores_list.append(pred_scores)\n",
    "\n",
    "    # Draw bounding boxes on image and save result\n",
    "    draw_bboxes(\n",
    "        output_dir_images, \n",
    "        Image.open(img_path), \n",
    "        os.path.splitext(image)[0], \n",
    "        prediction, fig_size, \n",
    "        CLASS_MAPPING, \n",
    "        saved_images_counter, \n",
    "        total_images=len(test_images)\n",
    "    )\n",
    "    \n",
    "    # Increment saved images counter\n",
    "    saved_images_counter += 1\n",
    "\n",
    "    # Closing all figures to free up memory\n",
    "    plt.close('all')\n",
    "        \n",
    "print(f\"Results Saved in {output_dir_images} Folder\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Metrics - Precision-Recall and Confusion Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code finalises the testing process by evaluating the model's performance and saving visualization plots. It first creates an output directory for storing evaluation plots, if it doesn't already exist. The collected true labels `(y_true_list)` and predicted scores `(y_scores_list)` are converted to NumPy arrays for compatibility with plotting functions.\n",
    "\n",
    "The `plot_precision_recall` function generates precision-recall curves and calculates the AUC for each class, saving the plots to the output directory. The `plot_multiclass_confusion_matrix` function creates a normalized confusion matrix to visualize classification performance across all classes. A completion message is printed to indicate the end of the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No positive samples found for class 'Background'. Skipping.\n",
      "Plotting Precision-Recall Curve and Calculating AUC | Done\n",
      "Plotting Confusion Matrix | Done\n",
      "\n",
      "----- <Testing Completed Successfully> -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add Output Directory for Plots\n",
    "if not os.path.exists(output_dir_plots):\n",
    "    os.makedirs(output_dir_plots)\n",
    "\n",
    "# Convert scores and predictions to NumPy arrays\n",
    "y_true_np = np.array(y_true_list)\n",
    "y_scores_np = np.array(y_scores_list)\n",
    "\n",
    "# Plot Precision-Recall Curve and AUC for each class\n",
    "plot_precision_recall(y_true_np, y_scores_np, numOfClasses, output_dir_plots, CLASS_MAPPING)\n",
    "# Plot Cconfusion Matrix\n",
    "plot_multiclass_confusion_matrix(y_true_np, y_scores_np, numOfClasses, output_dir_plots, CLASS_MAPPING, normalize=True)\n",
    "\n",
    "# Display completion message\n",
    "print(\"\\n----- <Testing Completed Successfully> -----\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
